{
	"name": "GetDataBrokerFiles_template_Schulung_Aaron",
	"properties": {
		"folder": {
			"name": "dmp_examples/DMP Schulung/User"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool0",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2fc583cc-9329-403c-8832-edc105bffd5b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/0494c897-f1d8-40e6-802f-a5d7b636b9f0/resourceGroups/rsgtrngprd1/providers/Microsoft.Synapse/workspaces/swatrngprd1/bigDataPools/sparkpool0",
				"name": "sparkpool0",
				"type": "Spark",
				"endpoint": "https://swatrngprd1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool0",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configure Spark\r\n",
					"Spark Session mit folgender Umgebungsvariable starten."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\r\n",
					"{\r\n",
					"    \"conf\":{\r\n",
					"        \"spark.rpc.message.maxSize\":\"2000\"\r\n",
					"    }\r\n",
					"}"
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Variablen definieren\r\n",
					"#### Speicherpfad\r\n",
					"- _container_name_: Container-Name des Storage Accounts\r\n",
					"- _account_name_: Storage Account Name, in welchem die Datei gespeichert werden soll\r\n",
					"- _folder_structure_: Die Ordner-Struktur, in welche die Dateien gespeichert werden sollen\r\n",
					"#### Databroker\r\n",
					"- _data_source_id_: Die DataSource des Databrokers, aus welcher die Liste aller zu herunterladenden Dateien erzeugt werden soll.\r\n",
					"- _date_from_: Frühester Erstellungszeitpunkt von Dateien in einer DataSource\r\n",
					"- _is_data_source_gzipped_: Erwartet in der DataSource nur .gz Dateien, entpackt diese und speichert die entpackten Resultate.\r\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"container_name = 'landing' # fill in your container name\r\n",
					"account_name = 'statrngprd1landing' # fill in your primary account name\r\n",
					"data_source_id = '898d998c-663d-489d-99e6-c1ba379e0113'\r\n",
					"folder_structure = 'user/Aaron'\r\n",
					"date_from = '1980-07-01' # Jahr 1980, führt dazu, dass der Filter quasi aus ist.\r\n",
					"is_data_source_gzipped = False"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Get Secrets\r\n",
					"Secrets für den Databroker aus dem KeyVault auslesen"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import sys\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"sc = SparkSession.builder.getOrCreate()\r\n",
					"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
					"\r\n",
					"keyvaultName = 'akvtrngprd1user'\r\n",
					"linkedServiceName = 'CustomerSecretsVault'\r\n",
					"\r\n",
					"databrokerClientId = token_library.getSecret(keyvaultName, \"databroker-client-id\", linkedServiceName)\r\n",
					"\r\n",
					"databrokerSecret = token_library.getSecret(keyvaultName, \"databroker-client-secret\", linkedServiceName)"
				],
				"attachments": null,
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Get Access Token\r\n",
					"Access Token für den Databroker mit den zuvor ausgelesenen Secrets beantragen"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import requests\r\n",
					"\r\n",
					"payload = {'client_id': databrokerClientId,'scope':'datasources', 'client_secret':databrokerSecret,'grant_type': 'client_credentials'}\r\n",
					"\r\n",
					"api_url = \"https://auth.businesshub.deutschebahn.com/auth/realms/data-broker/protocol/openid-connect/token\"\r\n",
					"result = requests.post(api_url, data=payload)\r\n",
					"access_token=result.json()['access_token']\r\n",
					"print(access_token)\r\n",
					"\r\n",
					"import datetime\r\n",
					"# Wir messen die Zeit, wann das Access Token erstellt wurde\r\n",
					"now = datetime.datetime.now()\r\n",
					""
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Get Filelist\r\n",
					"Gebe eine Liste mit allen Dateien aus, welche sich in einer DataSource befinden + filtert alte Dateien (date_from)"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"payload = {'dataSourceId': data_source_id,'dateFrom': date_from}\r\n",
					"headers = {\"Authorization\": \"Bearer \" + access_token}\r\n",
					"\r\n",
					"databroker_api_url = \"https://data-broker.service.deutschebahn.com/api/files-api/v1/files/\"\r\n",
					"file_list = requests.get(databroker_api_url, params=payload, headers=headers)\r\n",
					"\r\n",
					"files_to_download = [file_id['id'] for file_id in file_list.json()['files']] #nextToken muss bei langen Listen noch berücksichtigt werden\r\n",
					"print(files_to_download) "
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Download non-gz files from DataBroker"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not is_data_source_gzipped:\r\n",
					"    for file_id in files_to_download:\r\n",
					"        time_delta = datetime.datetime.now() - now\r\n",
					"        #check if jwt is not expired (renew every 20 mins)\r\n",
					"        if time_delta.total_seconds() < 1200:\r\n",
					"            api_url = \"https://data-broker.service.deutschebahn.com/api/files-api/v1/files/\" + file_id\r\n",
					"            headers = {\"Authorization\": \"Bearer \" + access_token}\r\n",
					"            file = requests.get(api_url, headers=headers)\r\n",
					"            adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/%s' % (container_name, account_name, folder_structure, file_id) #zukünftig noch metadaten auslesen\r\n",
					"            #chunk file in 500Mb pieces\r\n",
					"            for j in file.iter_content(500000000):\r\n",
					"                mssparkutils.fs.append(adls_path, str(j, 'utf-8'), True)\r\n",
					"        else:\r\n",
					"            # renew jwt token\r\n",
					"            payload = {'client_id': databrokerClientId,'scope':'datasources', 'client_secret':databrokerSecret,'grant_type': 'client_credentials'}\r\n",
					"            api_url = \"https://auth.businesshub.deutschebahn.com/auth/realms/data-broker/protocol/openid-connect/token\"\r\n",
					"            result = requests.post(api_url, data=payload)\r\n",
					"            access_token=result.json()['access_token']\r\n",
					"            now = datetime.datetime.now()"
				],
				"attachments": null,
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Download gz files from DataBroker"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import gzip\r\n",
					"\r\n",
					"if is_data_source_gzipped:\r\n",
					"    for file_id in files_to_download:\r\n",
					"        time_delta = datetime.datetime.now() - now\r\n",
					"        #check if jwt is not expired (renew every 20 mins)\r\n",
					"        if time_delta.total_seconds() < 1200:\r\n",
					"            api_url = \"https://databroker.service.deutschebahn.com/api/files-api/v1/files/\" + file_id\r\n",
					"            headers = {\"Authorization\": \"Bearer \" + access_token}\r\n",
					"            file = requests.get(api_url, headers=headers)\r\n",
					"            adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/%s' % (container_name, account_name, folder_structure, file_id) #zukünftig noch metadaten auslesen\r\n",
					"            data = gzip.decompress(file.content).decode('utf-8')\r\n",
					"            #chunk file in pieces\r\n",
					"            for i in range(0, len(data), 100000000):\r\n",
					"                mssparkutils.fs.append(adls_path, data[i:i+100000000], True)\r\n",
					"        else:\r\n",
					"            # renew jwt token\r\n",
					"            payload = {'client_id': databrokerClientId,'scope':'datasources', 'client_secret':databrokerSecret,'grant_type': 'client_credentials'}\r\n",
					"            api_url = \"https://auth.businesshub.deutschebahn.com/auth/realms/data-broker/protocol/openid-connect/token\"\r\n",
					"            result = requests.post(api_url, data=payload)\r\n",
					"            access_token=result.json()['access_token']\r\n",
					"            now = datetime.datetime.now()"
				],
				"attachments": null,
				"execution_count": 10
			}
		]
	}
}